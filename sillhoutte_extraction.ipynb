{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\h4rip/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been loaded.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "# Load pretrained model\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "# Segment people only for the purpose of human silhouette extraction\n",
    "people_class = 15\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "print (\"Model has been loaded.\")\n",
    "\n",
    "blur = torch.FloatTensor([[[[1.0, 2.0, 1.0],[2.0, 4.0, 2.0],[1.0, 2.0, 1.0]]]]) / 16.0\n",
    "\n",
    "# Use GPU if supported, for better performance\n",
    "if torch.cuda.is_available():\n",
    "\tmodel.to('cuda')\n",
    "\tblur = blur.to('cuda')\n",
    "\t\n",
    "# Apply preprocessing (normalization)\n",
    "preprocess = transforms.Compose([\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to create segmentation mask\n",
    "def makeSegMask(img):\n",
    "    # Scale input frame\n",
    "\tframe_data = torch.FloatTensor( img ) / 255.0\n",
    "\n",
    "\tinput_tensor = preprocess(frame_data.permute(2, 0, 1))\n",
    "    \n",
    "    # Create mini-batch to be used by the model\n",
    "\tinput_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Use GPU if supported, for better performance\n",
    "\tif torch.cuda.is_available():\n",
    "\t\tinput_batch = input_batch.to('cuda')\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(input_batch)['out'][0]\n",
    "\n",
    "\tsegmentation = output.argmax(0)\n",
    "\n",
    "\tbgOut = output[0:1][:][:]\n",
    "\ta = (1.0 - F.relu(torch.tanh(bgOut * 0.30 - 1.0))).pow(0.5) * 2.0\n",
    "\n",
    "\tpeople = segmentation.eq( torch.ones_like(segmentation).long().fill_(people_class) ).float()\n",
    "\n",
    "\tpeople.unsqueeze_(0).unsqueeze_(0)\n",
    "\t\n",
    "\tfor i in range(3):\n",
    "\t\tpeople = F.conv2d(people, blur, stride=1, padding=1)\n",
    "\n",
    "\t# Activation function to combine masks - F.hardtanh(a * b)\n",
    "\tcombined_mask = F.relu(F.hardtanh(a * (people.squeeze().pow(1.5)) ))\n",
    "\tcombined_mask = combined_mask.expand(1, 3, -1, -1)\n",
    "\n",
    "\tres = (combined_mask * 255.0).cpu().squeeze().byte().permute(1, 2, 0).numpy()\n",
    "\tthresh = 127\n",
    "\tim_bw = cv2.threshold(res, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "\treturn im_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done : 0\n",
      "Done : 4\n",
      "Done : 8\n",
      "Done : 12\n",
      "Done : 16\n",
      "Done : 20\n",
      "Done : 24\n",
      "Done : 28\n",
      "Done : 32\n",
      "Done : 36\n",
      "Done : 40\n",
      "Done : 44\n",
      "Done : 48\n",
      "Done : 52\n",
      "Done : 56\n",
      "Done : 60\n",
      "Done : 64\n",
      "Done : 68\n",
      "Done : 72\n",
      "Done : 76\n",
      "Done : 80\n",
      "Done : 84\n",
      "Done : 88\n",
      "Done : 92\n",
      "Done : 96\n",
      "Done : 100\n",
      "Done : 104\n",
      "Done : 108\n",
      "Done : 112\n",
      "Done : 116\n",
      "Done : 120\n",
      "Done : 124\n",
      "Done : 128\n",
      "Done : 132\n",
      "Done : 136\n",
      "Done : 140\n",
      "Done : 144\n",
      "Done : 148\n",
      "Done : 152\n",
      "Done : 156\n",
      "Done : 160\n",
      "Done : 164\n",
      "Done : 168\n",
      "Done : 172\n",
      "Done : 176\n",
      "Done : 180\n",
      "Done : 184\n",
      "Done : 188\n",
      "Done : 192\n",
      "Done : 196\n",
      "Done : 200\n",
      "Done : 204\n",
      "Done : 208\n",
      "Done : 212\n",
      "Done : 216\n",
      "Done : 220\n",
      "Done : 224\n",
      "Done : 228\n",
      "Done : 232\n",
      "Done : 236\n",
      "Done : 240\n",
      "Done : 244\n",
      "Done : 248\n",
      "Done : 252\n",
      "Done : 256\n",
      "Done : 260\n",
      "Done : 264\n",
      "Done : 268\n",
      "Done : 272\n",
      "Done : 276\n",
      "Done : 280\n",
      "Done : 284\n",
      "Done : 288\n",
      "Done : 292\n",
      "Done : 296\n",
      "Done : 300\n",
      "Done : 304\n",
      "Done : 308\n",
      "Done : 312\n",
      "Done : 316\n",
      "Done : 320\n",
      "Done : 324\n",
      "Done : 328\n",
      "Done : 332\n",
      "Done : 336\n",
      "Done : 340\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "vidcap = cv2.VideoCapture('./samples.mp4')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "\n",
    "frames = './test_dataset/frames/'\n",
    "masks = './test_dataset/masks/'\n",
    "\n",
    "while success:\n",
    "  if(count%4 == 0):\n",
    "    cv2.imwrite(frames+\"%d.jpg\" % count, image)     # save frame as JPEG file   \n",
    "    # cv2.imwrite(masks+str(count)+'.png', makeSegMask(cv2.resize((image), (256,256))))\n",
    "    print(\"Done : %s\" %count)   \n",
    "  success,image = vidcap.read()\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
