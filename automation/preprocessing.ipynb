{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from SemanticGuidedHumanMatting.model.model import HumanSegment, HumanMatting\n",
    "import SemanticGuidedHumanMatting.utils as utils\n",
    "import SemanticGuidedHumanMatting.inference as inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_file = './dataset/shilpa/6.mp4'\n",
    "id = 'shilpa_6'\n",
    "\n",
    "vidcap = cv2.VideoCapture(vid_file)\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "\n",
    "frames = './output/frames/'\n",
    "masks = './output/masks/'\n",
    "\n",
    "frame_files = glob.glob(frames + '/*')\n",
    "mask_files = glob.glob(masks + '/*')\n",
    "for f in frame_files:\n",
    "    os.remove(f)\n",
    "for f in mask_files:\n",
    "    os.remove(f)\n",
    "\n",
    "while success:\n",
    "  if(count%4 == 0):\n",
    "    image = cv2.resize(image, (int(image.shape[1]), int(image.shape[0])))\n",
    "    # print(int(image.shape[0]/8))\n",
    "    cv2.imwrite(frames+\"%d.jpg\" % count, image)  \n",
    "    # print(\"Done : %s\" %count)   \n",
    "  success,image = vidcap.read()\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 352x640 1 person, 714.8ms\n",
      "Speed: 1.0ms preprocess, 714.8ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 711.3ms\n",
      "Speed: 1.0ms preprocess, 711.3ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 689.8ms\n",
      "Speed: 1.0ms preprocess, 689.8ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 687.6ms\n",
      "Speed: 1.0ms preprocess, 687.6ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 681.0ms\n",
      "Speed: 1.0ms preprocess, 681.0ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1 handbag, 699.2ms\n",
      "Speed: 1.0ms preprocess, 699.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 666.4ms\n",
      "Speed: 0.0ms preprocess, 666.4ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 664.3ms\n",
      "Speed: 1.0ms preprocess, 664.3ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 772.7ms\n",
      "Speed: 0.0ms preprocess, 772.7ms inference, 7.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1032.5ms\n",
      "Speed: 1.6ms preprocess, 1032.5ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 970.8ms\n",
      "Speed: 0.0ms preprocess, 970.8ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 958.1ms\n",
      "Speed: 0.9ms preprocess, 958.1ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 991.0ms\n",
      "Speed: 1.0ms preprocess, 991.0ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 658.7ms\n",
      "Speed: 0.9ms preprocess, 658.7ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 922.3ms\n",
      "Speed: 0.9ms preprocess, 922.3ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 790.1ms\n",
      "Speed: 0.0ms preprocess, 790.1ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1 handbag, 813.8ms\n",
      "Speed: 1.0ms preprocess, 813.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1 handbag, 784.7ms\n",
      "Speed: 1.0ms preprocess, 784.7ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 767.8ms\n",
      "Speed: 0.0ms preprocess, 767.8ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 736.6ms\n",
      "Speed: 1.0ms preprocess, 736.6ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 723.1ms\n",
      "Speed: 0.0ms preprocess, 723.1ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 728.5ms\n",
      "Speed: 1.0ms preprocess, 728.5ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 722.4ms\n",
      "Speed: 1.0ms preprocess, 722.4ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 708.4ms\n",
      "Speed: 1.0ms preprocess, 708.4ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 (no detections), 697.7ms\n",
      "Speed: 1.0ms preprocess, 697.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# pretrained_weight = './SemanticGuidedHumanMatting/pretrained/SGHM-ResNet50.pth'\n",
    "# images_dir = './output/frames/'\n",
    "# gt_dir = ''\n",
    "# result_dir = './output/masks/'\n",
    "\n",
    "\n",
    "# if not os.path.exists(pretrained_weight):\n",
    "#     print('Cannot find the pretrained model: {0}'.format(pretrained_weight))\n",
    "#     exit()\n",
    "\n",
    "# model = HumanMatting(backbone='resnet50')\n",
    "# model = nn.DataParallel(model).eval()\n",
    "# model.load_state_dict(torch.load(pretrained_weight, map_location=torch.device('cpu')))\n",
    "# print(\"Load checkpoint successfully ...\")\n",
    "\n",
    "\n",
    "# image_list = sorted([*glob.glob(os.path.join(images_dir, '**', '*.jpg'), recursive=True),\n",
    "#                     *glob.glob(os.path.join(images_dir, '**', '*.png'), recursive=True)])\n",
    "\n",
    "# num_image = len(image_list)\n",
    "# print(\"Find \", num_image, \" images\")\n",
    "\n",
    "# for i in range(num_image):\n",
    "#     image_path = image_list[i]\n",
    "#     image_name = image_path[image_path.rfind('/')+1:image_path.rfind('.')]\n",
    "#     print(i, '/', num_image, image_name)\n",
    "\n",
    "#     with Image.open(image_path) as img:\n",
    "#         img = img.convert(\"RGB\")\n",
    "#         # img = img\n",
    "\n",
    "#     pred_alpha, pred_mask = inference.single_inference(model, img)\n",
    "\n",
    "#     output_dir = result_dir + image_path[len(images_dir):image_path.rfind('/')]\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "#     save_path = output_dir + '/' + os.path.split(image_name)[-1] + '.png'\n",
    "#     Image.fromarray(((pred_alpha * 255).astype('uint8')), mode='L').save(save_path)\n",
    "\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "pretrained_weight = './yolov8x-seg.pt'\n",
    "images_dir = './output/frames/'\n",
    "result_dir = './output/masks/'\n",
    "\n",
    "model = YOLO(pretrained_weight)\n",
    "\n",
    "files = os.listdir('./output/frames/')\n",
    "for f in files:\n",
    "    im = cv2.imread('./output/frames/'+'/'+f)\n",
    "    results = model(im)\n",
    "    masks = results[0].masks\n",
    "    if masks:\n",
    "        ms = masks.data.numpy()\n",
    "        cv2.imwrite(result_dir + f, ms[0,:,:]*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h4rip\\AppData\\Local\\Temp\\ipykernel_20208\\2402808924.py:46: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('./output/masks/')\n",
    "thresh = 127\n",
    "\n",
    "images = []\n",
    "for f in files:\n",
    "    im = cv2.imread('./output/masks/'+'/'+f, 0)\n",
    "    im_bw = cv2.threshold(im, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    images.append(im_bw)\n",
    "\n",
    "def mass_center(img,is_round=True):\n",
    "    Y = img.mean(axis=1)\n",
    "    X = img.mean(axis=0)\n",
    "    Y_ = np.sum(np.arange(Y.shape[0]) * Y)/np.sum(Y)\n",
    "    X_ = np.sum(np.arange(X.shape[0]) * X)/np.sum(X)\n",
    "    if is_round:\n",
    "        return int(round(X_)),int(round(Y_))\n",
    "    return X_,Y_\n",
    "\n",
    "def image_extract(img,newsize):\n",
    "    if (len(np.where(img.mean(axis=0)!=0)[0]) != 0):\n",
    "        x_s = np.where(img.mean(axis=0)!=0)[0].min()\n",
    "        x_e = np.where(img.mean(axis=0)!=0)[0].max()\n",
    "        \n",
    "        y_s = np.where(img.mean(axis=1)!=0)[0].min()\n",
    "        y_e = np.where(img.mean(axis=1)!=0)[0].max()\n",
    "        \n",
    "        x_c,_ = mass_center(img)\n",
    "        x_s = x_c-newsize[1]//2\n",
    "        x_e = x_c+newsize[1]//2\n",
    "        img = img[y_s:y_e,x_s if x_s>0 else 0:x_e if x_e<img.shape[1] else img.shape[1]]\n",
    "        return cv2.resize(img,newsize)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "image_data = []\n",
    "for k in images:\n",
    "    item = image_extract(k,(64, 128))\n",
    "    if(np.max(item) != 0):\n",
    "        image_data.append(item)\n",
    "\n",
    "gei = np.mean(image_data,axis=0)\n",
    "\n",
    "cv2.imwrite('./output/gei/'+id+'.png', gei)\n",
    "\n",
    "plt.imshow(gei)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "def aug_gen(img, out_path, img_name):\n",
    "    ia.seed(1)\n",
    "\n",
    "    # img = imageio.imread(\"test.jpg\") #read you image\n",
    "    images = np.array(\n",
    "        [img for _ in range(32)], dtype=np.uint8)  # 32 means creat 32 enhanced images using following methods.\n",
    "\n",
    "    seq = iaa.Sequential(\n",
    "        [            \n",
    "            iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.5))),        \n",
    "            iaa.ContrastNormalization((0.75, 1.5)),         \n",
    "            iaa.AdditiveGaussianNoise(\n",
    "                loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5),    \n",
    "        ],\n",
    "        random_order=True)  # apply augmenters in random order\n",
    "\n",
    "    images_aug = seq.augment_images(images)\n",
    "\n",
    "    for i in range(32):\n",
    "        imageio.imwrite(out_path + str(i)+ '_' + img_name + '.jpg', images_aug[i])  #write all changed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cv2.imread('./output/gei/abhirami/abhirami_0.png')\n",
    "\n",
    "out_path = './temp/'\n",
    "img_name = 'abhirami_0'\n",
    "\n",
    "aug_gen(test, out_path, img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h4rip\\anaconda3\\envs\\tf\\lib\\site-packages\\imgaug\\imgaug.py:184: DeprecationWarning: Function `ContrastNormalization()` is deprecated. Use `imgaug.contrast.LinearContrast` instead.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    }
   ],
   "source": [
    "in_path = './fine_tune_dataset/train/shilpa/'\n",
    "out_folder = 'aug/'\n",
    "\n",
    "files = os.listdir(in_path)\n",
    "os.mkdir(in_path + out_folder)\n",
    "\n",
    "for f in files:\n",
    "    im = cv2.imread(in_path+f, 0)\n",
    "    out_path = in_path + out_folder\n",
    "    img_name = f.split('.')[0]\n",
    "    aug_gen(im, out_path, img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
