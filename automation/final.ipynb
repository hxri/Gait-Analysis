{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_center(img,is_round=True):\n",
    "    Y = img.mean(axis=1)\n",
    "    X = img.mean(axis=0)\n",
    "    Y_ = np.sum(np.arange(Y.shape[0]) * Y)/np.sum(Y)\n",
    "    X_ = np.sum(np.arange(X.shape[0]) * X)/np.sum(X)\n",
    "    if is_round:\n",
    "        return int(round(X_)),int(round(Y_))\n",
    "    return X_,Y_\n",
    "\n",
    "def image_extract(img,newsize):\n",
    "    if (len(np.where(img.mean(axis=0)!=0)[0]) != 0):\n",
    "        x_s = np.where(img.mean(axis=0)!=0)[0].min()\n",
    "        x_e = np.where(img.mean(axis=0)!=0)[0].max()\n",
    "        \n",
    "        y_s = np.where(img.mean(axis=1)!=0)[0].min()\n",
    "        y_e = np.where(img.mean(axis=1)!=0)[0].max()\n",
    "        \n",
    "        x_c,_ = mass_center(img)\n",
    "        x_s = x_c-newsize[1]//2\n",
    "        x_e = x_c+newsize[1]//2\n",
    "        img = img[y_s:y_e,x_s if x_s>0 else 0:x_e if x_e<img.shape[1] else img.shape[1]]\n",
    "        return cv2.resize(img,newsize)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = {'Abhirami': 0,\n",
    "            'Aswathy': 1,\n",
    "            'Ayana': 2,\n",
    "            'Lekshmi': 3,\n",
    "            'Nandana': 4,\n",
    "            'Shilpa': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 352x640 1 person, 778.5ms\n",
      "Speed: 1.0ms preprocess, 778.5ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 759.9ms\n",
      "Speed: 1.0ms preprocess, 759.9ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 758.6ms\n",
      "Speed: 1.0ms preprocess, 758.6ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 744.3ms\n",
      "Speed: 1.0ms preprocess, 744.3ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 731.0ms\n",
      "Speed: 1.0ms preprocess, 731.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 796.2ms\n",
      "Speed: 1.0ms preprocess, 796.2ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 764.3ms\n",
      "Speed: 1.0ms preprocess, 764.3ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 732.3ms\n",
      "Speed: 1.9ms preprocess, 732.3ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1 toilet, 729.0ms\n",
      "Speed: 2.0ms preprocess, 729.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 352x640 1 person, 1 toilet, 789.3ms\n",
      "Speed: 1.9ms preprocess, 789.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 671ms/step\n",
      "[[  0.0075213  5.4802e-14  1.3305e-07   0.0020283     0.99043  1.8723e-05]]\n",
      "4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[  0.0075213  5.4802e-14  1.3305e-07   0.0020283     0.99043  1.8723e-05]]\n",
      "4\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[[  0.0075213  5.4802e-14  1.3305e-07   0.0020283     0.99043  1.8723e-05]]\n",
      "4\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.0075213  5.4802e-14  1.3305e-07   0.0020283     0.99043  1.8723e-05]]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 352x640 1 person, 1 toilet, 784.8ms\n",
      "Speed: 1.9ms preprocess, 784.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[ 0.00097672  4.6443e-15  2.4654e-07    0.001745     0.99728  7.3377e-07]]\n",
      "4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[ 0.00097672  4.6443e-15  2.4654e-07    0.001745     0.99728  7.3377e-07]]\n",
      "4\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[ 0.00097672  4.6443e-15  2.4654e-07    0.001745     0.99728  7.3377e-07]]\n",
      "4\n",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00097672  4.6443e-15  2.4654e-07    0.001745     0.99728  7.3377e-07]]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 352x640 1 person, 811.1ms\n",
      "Speed: 1.0ms preprocess, 811.1ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[   0.082248  2.7041e-14  1.9445e-06     0.12506     0.79269  5.2325e-06]]\n",
      "4\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[[   0.082248  2.7041e-14  1.9445e-06     0.12506     0.79269  5.2325e-06]]\n",
      "4\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[[   0.082248  2.7041e-14  1.9445e-06     0.12506     0.79269  5.2325e-06]]\n",
      "4\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.082248  2.7041e-14  1.9445e-06     0.12506     0.79269  5.2325e-06]]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 352x640 1 person, 775.5ms\n",
      "Speed: 1.0ms preprocess, 775.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[    0.48923  5.7902e-14  4.3471e-07     0.17744     0.33332  6.2815e-06]]\n",
      "0\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[    0.48923  5.7902e-14  4.3471e-07     0.17744     0.33332  6.2815e-06]]\n",
      "0\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[    0.48923  5.7902e-14  4.3471e-07     0.17744     0.33332  6.2815e-06]]\n",
      "0\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.48923  5.7902e-14  4.3471e-07     0.17744     0.33332  6.2815e-06]]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 352x640 1 person, 1 toilet, 740.6ms\n",
      "Speed: 1.0ms preprocess, 740.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[    0.85458   8.907e-14  3.7944e-07     0.12914    0.016238  4.4806e-05]]\n",
      "0\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[    0.85458   8.907e-14  3.7944e-07     0.12914    0.016238  4.4806e-05]]\n",
      "0\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[    0.85458   8.907e-14  3.7944e-07     0.12914    0.016238  4.4806e-05]]\n",
      "0\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.85458   8.907e-14  3.7944e-07     0.12914    0.016238  4.4806e-05]]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 352x640 1 person, 861.1ms\n",
      "Speed: 1.0ms preprocess, 861.1ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[    0.73871  1.0057e-13   9.454e-07     0.24782    0.013346  0.00012967]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "vid_file = './dataset/abhirami/6.mp4'\n",
    "\n",
    "vidcap = cv2.VideoCapture(vid_file)\n",
    "length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "mask_count = 0\n",
    "thresh = 127\n",
    "message = 'Analyzing...'\n",
    "m2 = ' '\n",
    "fps = 0\n",
    "\n",
    "pretrained_weight = './yolov8x-seg.pt'\n",
    "model = YOLO(pretrained_weight)\n",
    "new_model = load_model('./fine_tuned.h5')\n",
    "\n",
    "mask_dir = './output/masks/'\n",
    "mask_files = glob.glob(mask_dir+ '/*')\n",
    "for f in mask_files:\n",
    "    os.remove(f)\n",
    "\n",
    "while success:\n",
    "    start = time.time()\n",
    "    if(count%4 == 0):\n",
    "        results = model(image)\n",
    "        masks = results[0].masks\n",
    "        if masks:\n",
    "            m2 = 'YOLO v8 = Inference: {0} ms | Preprocess: {1} ms | Postprocess: {2} ms' .format(round(results[0].speed['inference'], 2), round(results[0].speed['preprocess'], 2), round(results[0].speed['postprocess'], 2))\n",
    "            mask_count = mask_count+1\n",
    "            ms = masks.data.numpy()\n",
    "            cv2.imwrite(mask_dir + str(count) + '.png', ms[0,:,:]*255)\n",
    "        \n",
    "    if(mask_count >= 10):\n",
    "        image_data = []\n",
    "        files = os.listdir('./output/masks/')\n",
    "        for f in files:\n",
    "            im = cv2.imread('./output/masks/'+'/'+f, 0)\n",
    "            im_bw = cv2.threshold(im, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "            item = image_extract(im_bw, (64, 128))\n",
    "            if(np.max(item) != 0):\n",
    "                image_data.append(item)\n",
    "        gei = np.mean(image_data,axis=0)\n",
    "\n",
    "        res_img = cv2.resize(gei, (224,224))\n",
    "        test_img = cv2.merge([res_img, res_img, res_img])\n",
    "        test_img = test_img/255\n",
    "        test_img = np.reshape(test_img, (1, 224, 224, 3))\n",
    "        \n",
    "        preds = new_model.predict(test_img)\n",
    "        print(preds)\n",
    "        prediction = np.argmax(preds)\n",
    "        print(prediction)\n",
    "        for name, label in lables.items():\n",
    "            if label == prediction:\n",
    "                message = 'Detected : ' + name + ' (' + str(round(preds[0][prediction]*100, 2)) + '% Accuracy)'\n",
    "\n",
    "    success,image = vidcap.read()\n",
    "    count += 1\n",
    "\n",
    "    mask_out = cv2.merge([ms[0,:,:]*255, ms[0,:,:]*255, ms[0,:,:]*255])\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    org = (25, 35)\n",
    "    org1 = (25, 60)\n",
    "    org2 = (25, 330)\n",
    "    org3 = (495, 320)\n",
    "    fontScale = 0.8\n",
    "    fontScale2 = 0.45\n",
    "    fontScale3 = 0.65\n",
    "    color = (0, 0, 0)\n",
    "    color2 = (0, 0, 255)\n",
    "    color3 = (255, 255, 255)\n",
    "    color4 = (255, 0, 255)\n",
    "    color5 = (0, 255, 0)\n",
    "    thickness = 2\n",
    "    thickness2 = 1\n",
    "\n",
    "    title1 = 'Live'\n",
    "    org4 = (25, 30)\n",
    "    title2 = 'YOLO v8 generated mask'\n",
    "\n",
    "    fps_msg = 'FPS: ' + str(round(fps/1000, 2))\n",
    "    image = cv2.putText(image, fps_msg, org3, font, \n",
    "                   fontScale3, color2, thickness, cv2.LINE_AA)\n",
    "    image = cv2.putText(image, title1, org4, font, \n",
    "                   fontScale3, color2, thickness, cv2.LINE_AA)\n",
    "    \n",
    "    mask_out = cv2.putText(mask_out, title2, org4, font, \n",
    "                   fontScale3, color4, thickness, cv2.LINE_AA)\n",
    "    \n",
    "    # mask_out = cv2.putText(mask_out, m2, org2, font, \n",
    "    #                 fontScale2, color5, thickness2, cv2.LINE_AA)\n",
    "    \n",
    "    prompt = np.zeros((95, 640, 3))*255\n",
    "    prompt = cv2.putText(prompt, message, org, font, \n",
    "                    fontScale, color5, thickness, cv2.LINE_AA)\n",
    "    prompt = cv2.putText(prompt, m2, org1, font, \n",
    "                    fontScale2, color3, thickness2, cv2.LINE_AA)\n",
    "    if(image is None):\n",
    "        cv2.destroyAllWindows()\n",
    "    if(image is not None):\n",
    "        # cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)/255\n",
    "        disp = np.concatenate((cv2.resize(image/255, (640, 280)), cv2.resize(mask_out, (640, 280))), axis=0)\n",
    "        info = np.concatenate((disp, prompt), axis=0)\n",
    "        cv2.imshow('Live', info)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "    end = time.time()\n",
    "    seconds = end - start \n",
    "    fps  = length / seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
